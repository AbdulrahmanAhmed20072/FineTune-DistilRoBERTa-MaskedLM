# FineTune-MaskedLM-DistilRoBERTa
This project fine-tunes DistilRoBERTa for Masked Language Modeling (MLM) using LoRA and BitsAndBytes quantization. It processes ELI5 data, tokenizes text, applies masked language modeling, and optimizes memory-efficient training using PEFT techniques.
